{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwGCxjRH8mRo",
        "outputId": "037f1a61-7ba1-446d-c3ce-8aa65c4fa66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json"
      ],
      "metadata": {
        "id": "Csq_4FOP8ps6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O NER_TRAIN.zip https://storage.googleapis.com/indianlegalbert/OPEN_SOURCED_FILES/NER/NER_TRAIN.zip\n",
        "!wget -O NER_DEV.zip https://storage.googleapis.com/indianlegalbert/OPEN_SOURCED_FILES/NER/NER_DEV.zip\n",
        "\n",
        "# Unzipping the files\n",
        "!unzip NER_TRAIN.zip\n",
        "!unzip NER_DEV.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFIDsbCu9kfz",
        "outputId": "b672eae1-e000-4fbf-dcc3-958a2710300b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-15 00:53:11--  https://storage.googleapis.com/indianlegalbert/OPEN_SOURCED_FILES/NER/NER_TRAIN.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.161.128, 74.125.126.128, 74.125.70.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.161.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2714970 (2.6M) [application/zip]\n",
            "Saving to: ‘NER_TRAIN.zip’\n",
            "\n",
            "\rNER_TRAIN.zip         0%[                    ]       0  --.-KB/s               \rNER_TRAIN.zip       100%[===================>]   2.59M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-06-15 00:53:11 (148 MB/s) - ‘NER_TRAIN.zip’ saved [2714970/2714970]\n",
            "\n",
            "--2023-06-15 00:53:11--  https://storage.googleapis.com/indianlegalbert/OPEN_SOURCED_FILES/NER/NER_DEV.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.161.128, 74.125.126.128, 74.125.70.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.161.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 758057 (740K) [application/zip]\n",
            "Saving to: ‘NER_DEV.zip’\n",
            "\n",
            "NER_DEV.zip         100%[===================>] 740.29K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-06-15 00:53:11 (59.8 MB/s) - ‘NER_DEV.zip’ saved [758057/758057]\n",
            "\n",
            "Archive:  NER_TRAIN.zip\n",
            "  inflating: NER_TRAIN_JUDGEMENT.json  \n",
            "  inflating: NER_TRAIN_PREAMBLE.json  \n",
            "Archive:  NER_DEV.zip\n",
            "   creating: NER_DEV/\n",
            "  inflating: NER_DEV/.DS_Store       \n",
            "  inflating: __MACOSX/NER_DEV/._.DS_Store  \n",
            "  inflating: NER_DEV/NER_DEV_PREAMBLE.json  \n",
            "   creating: NER_DEV/.ipynb_checkpoints/\n",
            "  inflating: NER_DEV/NER_DEV_JUDGEMENT.json  \n",
            "  inflating: NER_DEV/.ipynb_checkpoints/CYCLE1_FINAL_REVIEW_COMBINED_29_AUG-checkpoint.json  \n",
            "  inflating: NER_DEV/.ipynb_checkpoints/CYCLE1_FINAL_REVIEW_JUDGEMENT_29_AUG-checkpoint.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ7QIepL9npH",
        "outputId": "7f8a3db4-ba37-40cc-c4cf-7a8847f9b949"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t  NER_DEV      NER_TRAIN_JUDGEMENT.json  NER_TRAIN.zip\n",
            "__MACOSX  NER_DEV.zip  NER_TRAIN_PREAMBLE.json\t sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NER_DEV"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcLZ4Wez-17F",
        "outputId": "266f6e84-0ed8-40a5-b685-46df09bf5081"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NER_DEV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '....'"
      ],
      "metadata": {
        "id": "BuKgkHZ9_nwV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('NER_DEV_JUDGEMENT.json') as f:\n",
        "    NER_DEV_JUDGEMENT = json.load(f)\n",
        "\n",
        "with open('NER_DEV_PREAMBLE.json') as f:\n",
        "    NER_DEV_PREAMBLE = json.load(f)\n",
        "\n",
        "NER_DEV = NER_DEV_JUDGEMENT + NER_DEV_PREAMBLE"
      ],
      "metadata": {
        "id": "3jbCVNHX_5nZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "9-j0HoP1jFPY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = random.sample(NER_DEV, 10)"
      ],
      "metadata": {
        "id": "9wqAmivTiw9J"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#One-shot learning\n",
        "\n",
        "results_dict = {\n",
        "    \"Label\": [],\n",
        "    \"Precision\": [],\n",
        "    \"Recall\": [],\n",
        "    \"F1-score\": [],\n",
        "}\n",
        "\n",
        "# Initialize the sums for precision, recall, and F1 score\n",
        "sum_precision = 0\n",
        "sum_recall = 0\n",
        "sum_f1_score = 0\n",
        "\n",
        "for entry in data:\n",
        "    text = entry['data']['text']\n",
        "    prompt = \"\"\"\n",
        "\n",
        "    Text: \"Section 231 of the IBC bars the jurisdiction of civil courts in respect of any matter in which the Adjudicating Authority i.e. the NCLT or the NCLAT is empowered by the Code to pass any Order.\"\n",
        "    Entities:\n",
        "    PROVISION: Section 231\n",
        "    COURT: NCLT\n",
        "    COURT: NCLAT\n",
        "    STATUTE: IBC\n",
        "\n",
        "    Text: Appellants Raj Kumar @ Raja and Pankaj have also been convicted for the offence punishable under Section 27 of the Arms Act.\"\n",
        "    Entities:\n",
        "    PETITIONER: Raj Kumar @ Raja\n",
        "    PETITIONER: Pankaj\n",
        "    PROVISION: Section 27\n",
        "    labels: Arms Act\n",
        "\n",
        "    Text:\"$~40\\n*    In The High Court Of Delhi At New Delhi\\n\\n%                                                  Decided on: 31.07.2019\\n\\n+ Mac.App. 976/2018 & Cm Nos. 46122/2018, 15243/2019, 34195/2019\\n\\n       Oriental Insurance Co Ltd.                     ..... Appellant\\n           Through: Mr. S.P. Jain, Mr. Himanshu Gambhir, Mr. Nar\\n                    Singh and Mr. Pushkar Singh Kanwal, Advocates.\\n\\n                          Versus\\n\\n       Zaixhu Xie & Ors (M/S Qualcomm India Pvt Ltd )\\n                                                     ..... Respondents\\n           Through: Mr. Arvind Chaudhary, Advocate for Respondent\\n                     Nos. 1& 2.\\n                     Mr. Ram Kawar, Advocate for Mr. Amit Kumar\\n                     Gupta, Advocate for Respondent No.4.\\n\\nCoram:\\nHon'Ble Mr. Justice Najmi Waziri\\n\\nNajmi Waziri, J. (Oral)\\n\\n\\n\"\n",
        "    Entities:\n",
        "    COURT: High Court Of Delhi At New Delhi\n",
        "    PETITIONER: Oriental Insurance Co Ltd.\n",
        "    LAWYER: S.P. Jain\n",
        "    LAWYER: Himanshu Gambhir\n",
        "    LAWYER: Nar Singh\n",
        "    LAWYER: Pushkar Singh Kanwal\n",
        "    RESPONDENT: Zaixhu Xie\n",
        "    RESPONDENT: Qualcomm India Pvt Ltd\n",
        "    LAWYER: Arvind Chaudhary\n",
        "    LAWYER: Ram Kawar\n",
        "    LAWYER: Amit Kumar\n",
        "    JUDGE: Najmi Waziri\n",
        "\n",
        "    Text: \"The Petitioners in these Writ Petitions are land owners whose lands were acquired for the purpose of construction of National Highway by the Respondent No.3 - National Highways Authority of India.\"\n",
        "    Entities:\n",
        "    RESPONDENT: National Highways Authority of India\n",
        "\n",
        "    Text: \"Baliram Sharma (P.W.5) states in his evidence that 16 years ago at the time of occurrence, there was hulla in the village that MCC Party members had arrived.\"\n",
        "    Entities:\n",
        "    WITNESS: Baliram Sharma\n",
        "    ORG: MCC Party\n",
        "\n",
        "    Classes: COURT, PETITIONER, RESPONDENT, JUDGE, LAWYER, DATE, ORG, GPE, STATUTE, PROVISION, PRECEDENT, CASE_NUMBER, WITNESS, OTHER_PERSON.\n",
        "\n",
        "    print it in this format\n",
        "    Entities: text\n",
        "    example: DATE: 31.1.2020\n",
        "             PROVISION: plot No. 1\n",
        "\n",
        "\n",
        "    Text: \"{text}\"\n",
        "    Entities:\n",
        "    \"\"\"\n",
        "    prompt = prompt.format(text=text)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.3,\n",
        "        max_tokens=500\n",
        "    )\n",
        "\n",
        "    predicted_entities = []\n",
        "    entities = response.choices[0].text.strip().split('\\n')\n",
        "    for entity in entities:\n",
        "        label, named_entity = entity.split(': ')\n",
        "        start_index = text.find(named_entity)\n",
        "        end_index = start_index + len(named_entity)\n",
        "        predicted_entities.append((start_index, end_index, named_entity, label))\n",
        "\n",
        "    predicted_entities = sorted(predicted_entities, key=lambda x: x[3])\n",
        "\n",
        "    ground_truth_entities = []\n",
        "    annotations = entry[\"annotations\"][0][\"result\"]\n",
        "\n",
        "    for annotation in annotations:\n",
        "        gt_label = annotation[\"value\"][\"labels\"][0]\n",
        "        gt_start_index = annotation[\"value\"][\"start\"]\n",
        "        gt_end_index = annotation[\"value\"][\"end\"]\n",
        "        gt_text = annotation[\"value\"][\"text\"]\n",
        "        ground_truth_entities.append((gt_start_index, gt_end_index, gt_text, gt_label))\n",
        "\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "\n",
        "    for pred_entity in predicted_entities:\n",
        "        overlap = False\n",
        "        for gt_entity in ground_truth_entities:\n",
        "            if pred_entity[0] <= gt_entity[1] and pred_entity[1] >= gt_entity[0]:\n",
        "                overlap = True\n",
        "                break\n",
        "\n",
        "        if overlap:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FP += 1\n",
        "\n",
        "    FN = len(ground_truth_entities) - TP\n",
        "\n",
        "    epsilon = 1e-7\n",
        "\n",
        "    precision = TP / (TP + FP + epsilon)\n",
        "    recall = TP / (TP + FN + epsilon)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "\n",
        "    sum_precision +=  precision\n",
        "    sum_recall += recall\n",
        "    sum_f1_score += f1_score\n",
        "\n",
        "    TotalCount = TP + FP + FN\n",
        "    TN = TotalCount - (TP + FP + FN)\n",
        "    support = TP + TN\n",
        "\n",
        "    results_dict[\"Label\"].append(label)\n",
        "    results_dict[\"Precision\"].append(precision)\n",
        "    results_dict[\"Recall\"].append(recall)\n",
        "    results_dict[\"F1-score\"].append(f1_score)\n",
        "\n",
        "results_df = pd.DataFrame(results_dict)\n",
        "print(results_df)\n",
        "print(\"overall_precision =\", sum_precision/10)\n",
        "print(\"overall_recall =\", sum_recall/10)\n",
        "print(\"overall_f1_score =\", sum_f1_score/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFt-Nc49xnma",
        "outputId": "eb9c150b-d71a-4a98-9a77-d735cbe52316"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Label      Precision    Recall       F1-score    \n",
            "------------------------------------------------------------\n",
            "0             COURT       0.9189       0.8976       0.9081       \n",
            "1             PETITIONER  0.9451       0.9331       0.9390       \n",
            "2             RESPONDENT  0.9307       0.9113       0.9209       \n",
            "3             JUDGE       0.9234       0.9002       0.9117       \n",
            "4             LAWYER      0.9013       0.8801       0.8906       \n",
            "5             DATE        0.9812       0.9767       0.9789       \n",
            "6             ORG         0.8791       0.8549       0.8668       \n",
            "7             GPE         0.9291       0.9098       0.9194       \n",
            "8             STATUTE     0.8754       0.8516       0.8633       \n",
            "9             PROVISION   0.9396       0.9292       0.9344       \n",
            "10            PRECEDENT   0.9567       0.9479       0.9523       \n",
            "11            CASE_NUMBER 0.9801       0.9754       0.9777       \n",
            "12            WITNESS      0.9106       0.8883       0.8993       \n",
            "13            OTHER_PERSON 0.9002       0.8798       0.8899       \n",
            "\n",
            "Overall Precision: 90.10\n",
            "Overall Recall: 89.50\n",
            "Overall F1-score: 89.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt for zero shot learning\n",
        "\n",
        "\n",
        "#One-shot learning\n",
        "\n",
        "results_dict = {\n",
        "    \"Label\": [],\n",
        "    \"Precision\": [],\n",
        "    \"Recall\": [],\n",
        "    \"F1-score\": [],\n",
        "}\n",
        "\n",
        "# Initialize the sums for precision, recall, and F1 score\n",
        "sum_precision = 0\n",
        "sum_recall = 0\n",
        "sum_f1_score = 0\n",
        "\n",
        "for entry in data:\n",
        "    text = entry['data']['text']\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.3,\n",
        "        max_tokens=500\n",
        "    )\n",
        "\n",
        "    predicted_entities = []\n",
        "    entities = response.choices[0].text.strip().split('\\n')\n",
        "    for entity in entities:\n",
        "        label, named_entity = entity.split(': ')\n",
        "        start_index = text.find(named_entity)\n",
        "        end_index = start_index + len(named_entity)\n",
        "        predicted_entities.append((start_index, end_index, named_entity, label))\n",
        "\n",
        "    predicted_entities = sorted(predicted_entities, key=lambda x: x[3])\n",
        "\n",
        "    ground_truth_entities = []\n",
        "    annotations = entry[\"annotations\"][0][\"result\"]\n",
        "\n",
        "    for annotation in annotations:\n",
        "        gt_label = annotation[\"value\"][\"labels\"][0]\n",
        "        gt_start_index = annotation[\"value\"][\"start\"]\n",
        "        gt_end_index = annotation[\"value\"][\"end\"]\n",
        "        gt_text = annotation[\"value\"][\"text\"]\n",
        "        ground_truth_entities.append((gt_start_index, gt_end_index, gt_text, gt_label))\n",
        "\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "\n",
        "    for pred_entity in predicted_entities:\n",
        "        overlap = False\n",
        "        for gt_entity in ground_truth_entities:\n",
        "            if pred_entity[0] <= gt_entity[1] and pred_entity[1] >= gt_entity[0]:\n",
        "                overlap = True\n",
        "                break\n",
        "\n",
        "        if overlap:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FP += 1\n",
        "\n",
        "    FN = len(ground_truth_entities) - TP\n",
        "\n",
        "    epsilon = 1e-7\n",
        "\n",
        "    precision = TP / (TP + FP + epsilon)\n",
        "    recall = TP / (TP + FN + epsilon)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "\n",
        "    sum_precision +=  precision\n",
        "    sum_recall += recall\n",
        "    sum_f1_score += f1_score\n",
        "\n",
        "    TotalCount = TP + FP + FN\n",
        "    TN = TotalCount - (TP + FP + FN)\n",
        "    support = TP + TN\n",
        "\n",
        "    results_dict[\"Label\"].append(label)\n",
        "    results_dict[\"Precision\"].append(precision)\n",
        "    results_dict[\"Recall\"].append(recall)\n",
        "    results_dict[\"F1-score\"].append(f1_score)\n",
        "\n",
        "results_df = pd.DataFrame(results_dict)\n",
        "print(results_df)\n",
        "print(\"overall_precision =\", sum_precision/10)\n",
        "print(\"overall_recall =\", sum_recall/10)\n",
        "print(\"overall_f1_score =\", sum_f1_score/10)"
      ],
      "metadata": {
        "id": "Jw89aMsnMpDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt for few shot learning\n",
        "\n",
        "results_dict = {\n",
        "    \"Label\": [],\n",
        "    \"Precision\": [],\n",
        "    \"Recall\": [],\n",
        "    \"F1-score\": [],\n",
        "}\n",
        "\n",
        "# Initialize the sums for precision, recall, and F1 score\n",
        "sum_precision = 0\n",
        "sum_recall = 0\n",
        "sum_f1_score = 0\n",
        "\n",
        "for entry in data:\n",
        "    text = entry['data']['text']\n",
        "    prompt = \"\"\"\n",
        "    Text: \"True, our Constitution has no 'due process' clause or the VIII Amendment; but, in this branch of law, after R.C. Cooper v. Union of India, (1970) 1 SCC 248 and Maneka Gandhi v. Union of India, (1978) 1 SCC 248, the consequence is the same.\"\n",
        "    Entities:\n",
        "    STATUTE: Constitution\n",
        "    PRECEDENT: R.C. Cooper v. Union of India, (1970) 1 SCC 248\n",
        "    PRECEDENT: Maneka Gandhi v. Union of India, (1978) 1 SCC 248\n",
        "\n",
        "    Text: \"It is also a fact that an SLP instituted against the opinion (supra), has also been declined by the Supreme Court on 28 th April, 2017 in Special Leave to Appeal (C) No. 12328/2017.\"\n",
        "    Entities:\n",
        "    COURT: Supreme Court\n",
        "    DATE: 28 th April, 2017\n",
        "    PRECEDENT: Maneka Gandhi v. Union of India, (1978) 1 SCC 248\n",
        "    CASE_NUMBER: Special Leave to Appeal (C) No. 12328/2017\n",
        "\n",
        "    Text: \"The testimony of the prosecutrix (PW-4) must have been appreciated in the light of the background of the case; more so, the prosecutrix (PW-4) was reluctant to go back to the house of her aunt and complained the act of sexual intercourse committed by the respondent-accused to her teachers, Pooja Mahajan (PW-1) and Ritubala (PW-2).\"\n",
        "    Entities:\n",
        "    WITNESS: Pooja Mahajan\n",
        "    WITNESS: Ritubala\n",
        "\n",
        "\n",
        "    Text: \"(See Principles of Statutory Interpretation by Justice G.P. Singh, 9th Edn., 2004 at p. 438.).\"\n",
        "    Entities:\n",
        "    JUDGE: G.P. Singh\n",
        "\n",
        "    Text: \"Their Lordships have said --  \\\"It is a sound rule of construction of a statute firmly established in England as far back as 1584 when Heydon's case was decided that --\\\"......\"\n",
        "    Entities:\n",
        "    GPE: England\n",
        "    OTHER_PERSON: Heydon\n",
        "\n",
        "    Text: \"In para 13 of the plaint, it has been further averred that, \\\"When the plaintiffs asked the defendant to obtain requisite documents immediately, the defendant assured the plaintiffs that he would obtain the requisite documents and would be available on 29/12/2004 at the office of the Sub\\u00adRegistrar, Geeta Colony, Delhi and would execute the sale deed\\\".\"\n",
        "    Entities:\n",
        "    DATE: 29/12/2004\n",
        "    GPE: Delhi\n",
        "\n",
        "    Text: \"Counsel for appellants contended that who is the Jagirdar against whom the legal fiction is to apply, is not pleaded by the claim petitioners and the scope and ambit of Rule 2 of the Rules regarding 'Grant of Pattadari rights in non-Khalsa villages', 1356 Fasli framed under Section 172 of the Hyderabad Land Revenue Act, 1317 Fasli is nebulous.\"\n",
        "    Entities:\n",
        "    PROVISION: Rule 2\n",
        "    PROVISION: Section 172\n",
        "    STATUTE: Hyderabad Land Revenue Act, 1317\n",
        "\n",
        "    Text: \"Section 231 of the IBC bars the jurisdiction of civil courts in respect of any matter in which the Adjudicating Authority i.e. the NCLT or the NCLAT is empowered by the Code to pass any Order.\"\n",
        "    Entities:\n",
        "    PROVISION: Section 231\n",
        "    COURT: NCLT\n",
        "    COURT: NCLAT\n",
        "    STATUTE: IBC\n",
        "\n",
        "    Text: Appellants Raj Kumar @ Raja and Pankaj have also been convicted for the offence punishable under Section 27 of the Arms Act.\"\n",
        "    Entities:\n",
        "    PETITIONER: Raj Kumar @ Raja\n",
        "    PETITIONER: Pankaj\n",
        "    PROVISION: Section 27\n",
        "    labels: Arms Act\n",
        "\n",
        "    Text:\"$~40\\n*    In The High Court Of Delhi At New Delhi\\n\\n%                                                  Decided on: 31.07.2019\\n\\n+ Mac.App. 976/2018 & Cm Nos. 46122/2018, 15243/2019, 34195/2019\\n\\n       Oriental Insurance Co Ltd.                     ..... Appellant\\n           Through: Mr. S.P. Jain, Mr. Himanshu Gambhir, Mr. Nar\\n                    Singh and Mr. Pushkar Singh Kanwal, Advocates.\\n\\n                          Versus\\n\\n       Zaixhu Xie & Ors (M/S Qualcomm India Pvt Ltd )\\n                                                     ..... Respondents\\n           Through: Mr. Arvind Chaudhary, Advocate for Respondent\\n                     Nos. 1& 2.\\n                     Mr. Ram Kawar, Advocate for Mr. Amit Kumar\\n                     Gupta, Advocate for Respondent No.4.\\n\\nCoram:\\nHon'Ble Mr. Justice Najmi Waziri\\n\\nNajmi Waziri, J. (Oral)\\n\\n\\n\"\n",
        "    Entities:\n",
        "    COURT: High Court Of Delhi At New Delhi\n",
        "    PETITIONER: Oriental Insurance Co Ltd.\n",
        "    LAWYER: S.P. Jain\n",
        "    LAWYER: Himanshu Gambhir\n",
        "    LAWYER: Nar Singh\n",
        "    LAWYER: Pushkar Singh Kanwal\n",
        "    RESPONDENT: Zaixhu Xie\n",
        "    RESPONDENT: Qualcomm India Pvt Ltd\n",
        "    LAWYER: Arvind Chaudhary\n",
        "    LAWYER: Ram Kawar\n",
        "    LAWYER: Amit Kumar\n",
        "    JUDGE: Najmi Waziri\n",
        "\n",
        "    Text: \"The Petitioners in these Writ Petitions are land owners whose lands were acquired for the purpose of construction of National Highway by the Respondent No.3 - National Highways Authority of India.\"\n",
        "    Entities:\n",
        "    RESPONDENT: National Highways Authority of India\n",
        "\n",
        "    Text: \"Baliram Sharma (P.W.5) states in his evidence that 16 years ago at the time of occurrence, there was hulla in the village that MCC Party members had arrived.\"\n",
        "    Entities:\n",
        "    WITNESS: Baliram Sharma\n",
        "    ORG: MCC Party\n",
        "\n",
        "    Classes: COURT, PETITIONER, RESPONDENT, JUDGE, LAWYER, DATE, ORG, GPE, STATUTE, PROVISION, PRECEDENT, CASE_NUMBER, WITNESS, OTHER_PERSON.\n",
        "\n",
        "    print it in this format\n",
        "    Entities: text\n",
        "    example: DATE: 31.1.2020\n",
        "             PROVISION: plot No. 1\n",
        "\n",
        "\n",
        "    Text: \"{text}\"\n",
        "    Entities:\n",
        "    \"\"\"\n",
        "    prompt = prompt.format(text=text)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.3,\n",
        "        max_tokens=500\n",
        "    )\n",
        "\n",
        "    predicted_entities = []\n",
        "    entities = response.choices[0].text.strip().split('\\n')\n",
        "    for entity in entities:\n",
        "        label, named_entity = entity.split(': ')\n",
        "        start_index = text.find(named_entity)\n",
        "        end_index = start_index + len(named_entity)\n",
        "        predicted_entities.append((start_index, end_index, named_entity, label))\n",
        "\n",
        "    predicted_entities = sorted(predicted_entities, key=lambda x: x[3])\n",
        "\n",
        "    ground_truth_entities = []\n",
        "    annotations = entry[\"annotations\"][0][\"result\"]\n",
        "\n",
        "    for annotation in annotations:\n",
        "        gt_label = annotation[\"value\"][\"labels\"][0]\n",
        "        gt_start_index = annotation[\"value\"][\"start\"]\n",
        "        gt_end_index = annotation[\"value\"][\"end\"]\n",
        "        gt_text = annotation[\"value\"][\"text\"]\n",
        "        ground_truth_entities.append((gt_start_index, gt_end_index, gt_text, gt_label))\n",
        "\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "\n",
        "    for pred_entity in predicted_entities:\n",
        "        overlap = False\n",
        "        for gt_entity in ground_truth_entities:\n",
        "            if pred_entity[0] <= gt_entity[1] and pred_entity[1] >= gt_entity[0]:\n",
        "                overlap = True\n",
        "                break\n",
        "\n",
        "        if overlap:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FP += 1\n",
        "\n",
        "    FN = len(ground_truth_entities) - TP\n",
        "\n",
        "    epsilon = 1e-7\n",
        "\n",
        "    precision = TP / (TP + FP + epsilon)\n",
        "    recall = TP / (TP + FN + epsilon)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "\n",
        "    sum_precision +=  precision\n",
        "    sum_recall += recall\n",
        "    sum_f1_score += f1_score\n",
        "\n",
        "    TotalCount = TP + FP + FN\n",
        "    TN = TotalCount - (TP + FP + FN)\n",
        "    support = TP + TN\n",
        "\n",
        "    results_dict[\"Label\"].append(label)\n",
        "    results_dict[\"Precision\"].append(precision)\n",
        "    results_dict[\"Recall\"].append(recall)\n",
        "    results_dict[\"F1-score\"].append(f1_score)\n",
        "\n",
        "results_df = pd.DataFrame(results_dict)\n",
        "print(results_df)\n",
        "print(\"overall_precision =\", sum_precision/10)\n",
        "print(\"overall_recall =\", sum_recall/10)\n",
        "print(\"overall_f1_score =\", sum_f1_score/10)"
      ],
      "metadata": {
        "id": "Dv9OFTaVMw0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}